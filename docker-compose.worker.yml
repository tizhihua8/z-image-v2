# ============================================
# Z-Image v3 Worker Docker Compose 配置
# ============================================
# 此文件用于独立部署 GPU Worker
# 使用方法:
# 1. 确保已安装 NVIDIA Container Toolkit
# 2. 配置 .env 文件中的 WORKER_API_KEY 和 SERVER_URL
# 3. 启动: docker-compose -f docker-compose.worker.yml up -d

version: '3.8'

services:
  # ============================================
  # AI Worker (GPU 加速)
  # ============================================
  worker:
    build:
      context: ./worker
      dockerfile: Dockerfile
    container_name: zimage-worker
    restart: unless-stopped
    # GPU 配置
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu, compute, utility]
    environment:
      # Worker 标识
      - WORKER_ID=${WORKER_ID:-worker-docker}
      - WORKER_NAME=${WORKER_NAME:-Docker Worker}
      # 服务器连接
      - REMOTE_API_BASE=${SERVER_URL:-https://your-domain.com}
      - WORKER_API_KEY=${WORKER_API_KEY}
      # 模型配置
      - MODEL_ID=${MODEL_ID:-Tongyi-MAI/Z-Image-Turbo}
      - DEVICE=cuda
      - USE_CPU_OFFLOAD=${USE_CPU_OFFLOAD:-true}
      # 本地备份
      - LOCAL_BACKUP_ROOT=/worker/backup
      # 模型缓存
      - HF_HOME=/worker/models
      - TRANSFORMERS_CACHE=/worker/models
    volumes:
      # 模型缓存（避免重复下载）
      - worker-models:/worker/models
      # 本地备份
      - worker-backup:/worker/backup
    networks:
      - zimage-worker-network
    # 共享内存（重要：大模型需要）
    shm_size: '8gb'

# ============================================
# 网络配置
# ============================================
networks:
  zimage-worker-network:
    driver: bridge
    name: zimage-worker-network

# ============================================
# 数据卷
# ============================================
volumes:
  worker-models:
    driver: local
    name: worker-models
  worker-backup:
    driver: local
    name: worker-backup
